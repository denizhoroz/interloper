{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345e500",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'initialize_model' from 'interloper' (d:\\projects\\interloper\\model\\interloper.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minterloper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_model, Session, Evaluator\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'initialize_model' from 'interloper' (d:\\projects\\interloper\\model\\interloper.py)"
     ]
    }
   ],
   "source": [
    "from interloper import initialize_model, Session, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = initialize_model(model_path=\"../models/llama-3-8b-instruct-Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88329c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   29172.58 ms\n",
      "llama_perf_context_print: prompt eval time =   29172.06 ms /   378 tokens (   77.17 ms per token,    12.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4043.16 ms /    11 runs   (  367.56 ms per token,     2.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   33569.88 ms /   389 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<CONTINUE>',\n",
       " 'Good day! Can I take your order, please?',\n",
       " [HumanMessage(content='', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Good day! Can I take your order, please?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = Session(model=llm, session_n=1)\n",
    "\n",
    "session.generate_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d88a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 337 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   29172.58 ms\n",
      "llama_perf_context_print: prompt eval time =   21642.01 ms /   230 tokens (   94.10 ms per token,    10.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10405.38 ms /    34 runs   (  306.04 ms per token,     3.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   32284.78 ms /   264 tokens\n",
      "llama_perf_context_print:    graphs reused =         32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<CONTINUE>',\n",
       " 'We have a variety of dishes to choose from. Our specials include grilled salmon, chicken parmesan, and vegetarian quinoa bowl.',\n",
       " [HumanMessage(content='', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Good day! Can I take your order, please?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Yeah, what do you have on the menu?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='We have a variety of dishes to choose from. Our specials include grilled salmon, chicken parmesan, and vegetarian quinoa bowl.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.generate_message(input=\"Yeah, what do you have on the menu?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de8980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 357 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   29172.58 ms\n",
      "llama_perf_context_print: prompt eval time =   18860.94 ms /   265 tokens (   71.17 ms per token,    14.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12881.96 ms /    47 runs   (  274.08 ms per token,     3.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   31873.22 ms /   312 tokens\n",
      "llama_perf_context_print:    graphs reused =         45\n"
     ]
    }
   ],
   "source": [
    "status, output, session_history = session.generate_message(input=\"I'll take a grilled salmon and a vegetarian quino bowl. And a mineral water.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28fd79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<CONTINUE>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b16657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll get that prepared for you. One grilled salmon and one vegetarian quinoa bowl, both served with a side of mineral water. Would you like to make any changes or additions before I put your order in?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f610afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Good day! Can I take your order, please?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Yeah, what do you have on the menu?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='We have a variety of dishes to choose from. Our specials include grilled salmon, chicken parmesan, and vegetarian quinoa bowl.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I'll take a grilled salmon and a vegetarian quino bowl. And a mineral water.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'll get that prepared for you. One grilled salmon and one vegetarian quinoa bowl, both served with a side of mineral water. Would you like to make any changes or additions before I put your order in?\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86632299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\interloper\\model\\interloper.py:169: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  evaluation_chain = LLMChain(\n",
      "Llama.generate: 4 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   29172.58 ms\n",
      "llama_perf_context_print: prompt eval time =   16094.62 ms /   217 tokens (   74.17 ms per token,    13.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45560.90 ms /   182 runs   (  250.33 ms per token,     3.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   62181.77 ms /   399 tokens\n",
      "llama_perf_context_print:    graphs reused =        175\n"
     ]
    }
   ],
   "source": [
    "from interloper import Evaluator\n",
    "\n",
    "evaluator = Evaluator(model=llm)\n",
    "\n",
    "results = evaluator.evaluate(session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1736300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\interloper\\venv\\lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import argostranslate.package, argostranslate.translate, pathlib\n",
    "\n",
    "package_path = pathlib.Path(\"../models/translate-en_tr-1_5.argosmodel\")\n",
    "argostranslate.package.install_from_path(package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3182938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba, nasılsın?\n"
     ]
    }
   ],
   "source": [
    "import argostranslate.translate\n",
    "\n",
    "translated = argostranslate.translate.translate(\"Hello, how are you?\", \"en\", \"tr\")\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3a13640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_argos_model(model_path):\n",
    "    package_path = pathlib.Path(model_path)\n",
    "    argostranslate.package.install_from_path(package_path)\n",
    "\n",
    "def translate_text(text: str, source_lang=\"en\", target_lang=\"tr\") -> str:\n",
    "    return argostranslate.translate.translate(text, source_lang, target_lang)\n",
    "\n",
    "def translate_conversation(conversation, target_lang=\"tr\"):\n",
    "    translated_conv = []\n",
    "    conversation = conversation.split('\\n')\n",
    "    for line in conversation:\n",
    "        speaker, content = line.split(\":\", 1)\n",
    "        translated = translate_text(content.strip(), \"en\", target_lang)\n",
    "        translated_conv.append(f\"{speaker}: {translated}\")\n",
    "    return translated_conv\n",
    "\n",
    "def translate_evaluation(evaluation, target_lang=\"tr\"):\n",
    "    translated_eval = {}\n",
    "    for section, data in evaluation.items():\n",
    "        translated_eval[section] = {\n",
    "            \"score\": data[\"score\"],  # keep score unchanged\n",
    "            \"comment\": translate_text(data[\"comment\"], \"en\", target_lang)\n",
    "        }\n",
    "    return translated_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82b8f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Good day! Can I take your order, please?\n",
      "HUMAN: Yeah, what do you have on the menu?\n",
      "AI: We have a variety of dishes to choose from. Our specials include grilled salmon, chicken parmesan, and vegetarian quinoa bowl.\n",
      "HUMAN: I'll take a grilled salmon and a vegetarian quino bowl. And a mineral water.\n",
      "AI: I'll get that prepared for you. One grilled salmon and one vegetarian quinoa bowl, both served with a side of mineral water. Would you like to make any changes or additions before I put your order in?\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.parse_history(session_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79277217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI: İyi gün! Can Can Can Can Can Can Can Siparişinizi alıyorum, lütfen?',\n",
       " 'HUMAN: Evet, menüde ne var?',\n",
       " 'AI: Seçim yapmak için çeşitli yemeklerimiz var. Özellerimiz ızgara somon, tavuk parmesan ve vejetaryen quinoa kase içerir.',\n",
       " 'HUMAN: Bir ızgara somon ve bir vejetaryen quino kase alacağım. Ve bir mineral suyu.',\n",
       " 'AI: Bunu sizin için hazırlayacağım. Bir ızgara somon ve bir vejetaryen quinoa kase, her ikisi de bir maden suyu ile hizmet etti. Siparişinizi koymadan önce herhangi bir değişiklik veya ekleme yapmak ister misiniz?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_conversation(evaluator.parse_history(session_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91fda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
